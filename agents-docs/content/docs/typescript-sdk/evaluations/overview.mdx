---
title: Evaluations Overview
sidebarTitle: Overview
description: Comprehensive evaluation system for testing and measuring agent performance with datasets, evaluators, and automated testing.
icon: LuClipboardCheck
keywords: evaluations, testing, datasets, evaluators, agent performance, quality assurance
---

## What is the Evaluation System?

The Inkeep Agent Framework includes a powerful evaluation system (`agents-eval-api`) that enables you to systematically test, measure, and improve your AI agents. The evaluation system provides:

- **Dataset Management**: Create and manage test datasets with input/output pairs
- **Evaluator Configuration**: Define custom evaluation criteria using LLM-as-a-judge
- **Automated Testing**: Run evaluations automatically on conversations
- **Multi-turn Simulations**: Test complex multi-turn conversations with simulation agents
- **Result Tracking**: Store and analyze evaluation results over time
- **Integration with Traces**: Evaluate using full execution traces from SigNoz

## Key Concepts

### Datasets

A dataset is a collection of test cases (dataset items) that represent scenarios you want to test your agent against. Each dataset item contains:

- **Input**: Messages to send to the agent (can be single or multi-turn)
- **Expected Output**: What you expect the agent to produce (optional)
- **Metadata**: Additional context like tags, categories, difficulty level

### Dataset Items

Individual test cases within a dataset. Each item can include:

- User messages to send to the agent
- Optional simulation agent configuration for multi-turn conversations
- Expected outputs for comparison
- Custom metadata for filtering and organization

### Evaluators

Evaluators are LLM-powered judges that assess agent responses based on specific criteria. Each evaluator has:

- **Prompt**: Instructions for the evaluation LLM
- **Schema**: JSON schema defining the evaluation output structure
- **Model**: Which LLM model to use for evaluation (e.g., GPT-4, Claude)

### Evaluation Suites

Collections of evaluators that are commonly run together. For example, you might have a "Quality Suite" that includes evaluators for:

- Response accuracy
- Tone appropriateness
- Completeness
- Hallucination detection

### Evaluation Runs

An execution of evaluations against a set of conversations. Each run tracks:

- Which evaluators were used
- Which conversations were evaluated
- Timestamps and duration
- All evaluation results

### Dataset Runs

An execution of a dataset against an agent, creating conversations that can then be evaluated. Dataset runs:

- Execute each dataset item against the configured agent
- Create conversation records for each test case
- Support multi-turn simulations
- Link conversations to evaluation jobs

## Architecture

The evaluation system consists of several components:

```
┌─────────────────┐
│  Eval API       │  (Port 3005)
│  agents-eval-api│
└────────┬────────┘
         │
         ├──► Manages datasets & evaluators
         ├──► Runs evaluations via Inngest
         ├──► Stores results in database
         └──► Fetches traces from SigNoz
              
┌─────────────────┐
│  Run API        │  (Port 3003)
│  agents-run-api │
└────────┬────────┘
         │
         └──► Executes agent conversations
              for dataset runs

┌─────────────────┐
│  Manage UI      │  (Port 3000)
│agents-manage-ui │
└────────┬────────┘
         │
         ├──► Create/edit datasets
         ├──► Configure evaluators
         └──► View evaluation results
```

## Use Cases

### Regression Testing

Create a dataset of important test cases and run evaluations automatically to ensure new changes don't break existing functionality:

```typescript
// Run evaluations on every deployment
const results = await runDatasetWithEvaluations({
  datasetId: 'regression-suite',
  agentId: 'production-agent',
  evaluationSuiteId: 'quality-checks'
});
```

### A/B Testing

Compare different agent configurations by running the same dataset against multiple agents:

```typescript
// Test two different prompts
const resultsA = await runDataset('test-dataset', 'agent-prompt-a');
const resultsB = await runDataset('test-dataset', 'agent-prompt-b');

// Compare evaluation scores
compareResults(resultsA, resultsB);
```

### Quality Monitoring

Continuously evaluate production conversations to monitor agent quality:

```typescript
// Evaluate recent conversations
await runEvaluationJob({
  jobFilters: {
    dateRange: {
      startDate: last24Hours,
      endDate: now
    }
  },
  evaluators: ['hallucination-check', 'tone-checker']
});
```

### Multi-turn Testing

Test complex conversational flows with simulation agents:

```typescript
const datasetItem = {
  input: { messages: [{ role: 'user', content: 'Hello' }] },
  simulationAgent: {
    prompt: 'You are a confused customer asking follow-up questions',
    model: { model: 'gpt-4o-mini' },
    stopWhen: { stepCountIs: 5 }
  }
};
```

## Environment Setup

The evaluation API requires the following environment variables:

```bash
# Evaluation API URL
AGENTS_EVAL_API_URL=http://localhost:3005

# Run API URL (for executing agent conversations)
AGENTS_RUN_API_URL=http://localhost:3003

# Manage UI URL (for fetching traces)
AGENTS_MANAGE_UI_URL=http://localhost:3000

# Database connection
DATABASE_URL=postgresql://user:password@localhost:5432/inkeep_agents

# Logging level
LOG_LEVEL=info

# Optional: bypass API key auth for local development
INKEEP_AGENTS_EVAL_API_BYPASS_SECRET=your-secret-key

# Inngest configuration (for background jobs)
INNGEST_EVENT_KEY=your-event-key
INNGEST_SIGNING_KEY=your-signing-key
INNGEST_DEV=true
```

## Starting the Evaluation API

```bash
# From the monorepo root
cd agents-eval-api

# Install dependencies
pnpm install

# Run in development mode
pnpm dev

# The API will be available at http://localhost:3005
```

## API Documentation

Once the evaluation API is running, you can access interactive API documentation:

- **Swagger UI**: http://localhost:3005/docs
- **OpenAPI Spec**: http://localhost:3005/openapi.json

## Next Steps

<Cards>
  <Card title="Datasets" icon="LuDatabase" href="/typescript-sdk/evaluations/datasets">
    Learn how to create and manage test datasets
  </Card>
  <Card title="Evaluators" icon="LuScale" href="/typescript-sdk/evaluations/evaluators">
    Configure LLM-powered evaluators to judge agent performance
  </Card>
  <Card title="Running Evaluations" icon="LuPlay" href="/typescript-sdk/evaluations/running-evaluations">
    Execute evaluations and analyze results
  </Card>
  <Card title="Simulation Agents" icon="LuUsers" href="/typescript-sdk/evaluations/simulation-agents">
    Create multi-turn conversations with simulation agents
  </Card>
</Cards>

