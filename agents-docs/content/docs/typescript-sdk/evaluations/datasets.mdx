---
title: Datasets and Dataset Items
sidebarTitle: Datasets
description: Create and manage test datasets for evaluating your AI agents with single and multi-turn conversations.
icon: LuDatabase
keywords: datasets, test data, dataset items, agent testing, test cases
---

## What are Datasets?

Datasets are collections of test cases used to evaluate your AI agents. Each dataset contains multiple dataset items representing different scenarios you want to test.

## Creating a Dataset

### Via API

```typescript
POST /tenants/{tenantId}/projects/{projectId}/evaluations/datasets

{
  "name": "Customer Support Tests",
  "description": "Test cases for customer support agent",
  "metadata": {
    "category": "support",
    "priority": "high"
  }
}
```

### Via Management UI

1. Navigate to **Evaluations** → **Datasets**
2. Click **Create Dataset**
3. Enter name and description
4. Add metadata tags (optional)
5. Click **Save**

## Dataset Items

Dataset items are individual test cases within a dataset. Each item defines:

- **Input**: Messages to send to the agent
- **Expected Output**: What you expect the agent to produce (optional)
- **Simulation Agent**: Configuration for multi-turn testing (optional)
- **Metadata**: Tags, difficulty level, category, etc.

## Creating Dataset Items

### Simple Single-turn Item

```typescript
POST /tenants/{tenantId}/projects/{projectId}/evaluations/datasets/{datasetId}/items

{
  "input": {
    "messages": [
      {
        "role": "user",
        "content": "What is your return policy?"
      }
    ]
  },
  "expectedOutput": {
    "shouldMention": ["30 days", "receipt required", "refund"]
  },
  "metadata": {
    "difficulty": "easy",
    "category": "policy"
  }
}
```

### Multi-turn with Simulation Agent

```typescript
{
  "input": {
    "messages": [
      {
        "role": "user",
        "content": "I need help with my order"
      }
    ]
  },
  "simulationAgent": {
    "prompt": "You are a frustrated customer whose order is late. Ask follow-up questions about tracking and delivery.",
    "model": {
      "model": "gpt-4o-mini",
      "providerOptions": {
        "temperature": 0.7
      }
    },
    "stopWhen": {
      "stepCountIs": 5,
      "transferCountIs": 2
    }
  },
  "metadata": {
    "difficulty": "hard",
    "category": "escalation"
  }
}
```

## Input Message Formats

Dataset items support flexible message formats:

### Basic String Input

```typescript
{
  "input": "Hello, how can you help me?"
}
```

This is automatically converted to:

```typescript
{
  "input": {
    "messages": [
      { "role": "user", "content": "Hello, how can you help me?" }
    ]
  }
}
```

### Multi-message Conversation

```typescript
{
  "input": {
    "messages": [
      { "role": "system", "content": "You are a helpful assistant" },
      { "role": "user", "content": "Tell me about your products" },
      { "role": "assistant", "content": "We offer three product lines..." },
      { "role": "user", "content": "What about pricing?" }
    ]
  }
}
```

### Valid Message Roles

- `user`: User messages
- `assistant` or `agent`: Agent responses (both map to `assistant` internally)
- `system`: System prompts
- `function`: Function call results (for tool usage)
- `tool`: Tool execution results

## Expected Outputs

Expected outputs help you define what a successful response should contain:

### Keyword Matching

```typescript
{
  "expectedOutput": {
    "shouldContain": ["refund", "30 days"],
    "shouldNotContain": ["no refunds", "non-returnable"]
  }
}
```

### Structured Output

```typescript
{
  "expectedOutput": {
    "intent": "return_request",
    "entities": {
      "orderId": "12345",
      "reason": "defective"
    },
    "nextAction": "initiate_return"
  }
}
```

### Free-form Description

```typescript
{
  "expectedOutput": {
    "description": "Agent should acknowledge the issue, ask for order number, and explain the return process clearly"
  }
}
```

## Simulation Agents

Simulation agents enable multi-turn conversation testing by automatically generating follow-up messages.

### Configuration Options

```typescript
{
  "simulationAgent": {
    // Persona prompt for the simulation agent
    "prompt": "You are a customer persona description...",
    
    // Model to use for generating responses
    "model": {
      "model": "gpt-4o-mini",
      "providerOptions": {
        "temperature": 0.8,
        "maxTokens": 200
      }
    },
    
    // When to stop the conversation
    "stopWhen": {
      // Stop after N conversation turns
      "stepCountIs": 10,
      
      // Stop after N agent transfers (optional)
      "transferCountIs": 3
    }
  }
}
```

### Example Simulation Personas

**Confused Customer**:
```typescript
{
  "prompt": "You are a confused customer who doesn't understand technical jargon. Ask clarifying questions and request simpler explanations.",
  "model": { "model": "gpt-4o-mini" },
  "stopWhen": { "stepCountIs": 5 }
}
```

**Angry Customer**:
```typescript
{
  "prompt": "You are an angry customer whose order is 2 weeks late. Be frustrated but listen to solutions. Gradually calm down if the agent handles the situation well.",
  "model": { "model": "gpt-4o" },
  "stopWhen": { "stepCountIs": 8 }
}
```

**Detail-oriented Customer**:
```typescript
{
  "prompt": "You are a very detail-oriented customer who asks specific questions about features, specifications, and policies. You want precise answers.",
  "model": { "model": "claude-3-5-sonnet-20241022" },
  "stopWhen": { "stepCountIs": 6 }
}
```

## Metadata

Use metadata to organize and filter dataset items:

```typescript
{
  "metadata": {
    // Difficulty level
    "difficulty": "easy" | "medium" | "hard",
    
    // Category or topic
    "category": "returns" | "technical" | "billing",
    
    // Expected duration
    "expectedDuration": "short" | "medium" | "long",
    
    // Tags for filtering
    "tags": ["regression", "critical", "edge-case"],
    
    // Custom fields
    "priority": 1,
    "language": "en",
    "region": "US"
  }
}
```

## Bulk Import

Import multiple dataset items at once:

```typescript
POST /tenants/{tenantId}/projects/{projectId}/evaluations/datasets/{datasetId}/items/bulk

{
  "items": [
    {
      "input": { "messages": [{ "role": "user", "content": "Test 1" }] },
      "metadata": { "id": "test-1" }
    },
    {
      "input": { "messages": [{ "role": "user", "content": "Test 2" }] },
      "metadata": { "id": "test-2" }
    }
  ]
}
```

## Managing Datasets

### List Datasets

```typescript
GET /tenants/{tenantId}/projects/{projectId}/evaluations/datasets

// Response
{
  "data": [
    {
      "id": "dataset_abc123",
      "name": "Customer Support Tests",
      "description": "Test cases for support agent",
      "itemCount": 50,
      "createdAt": "2024-01-01T00:00:00Z",
      "updatedAt": "2024-01-15T00:00:00Z"
    }
  ]
}
```

### Update Dataset

```typescript
PATCH /tenants/{tenantId}/projects/{projectId}/evaluations/datasets/{datasetId}

{
  "name": "Updated Name",
  "description": "Updated description"
}
```

### Delete Dataset

```typescript
DELETE /tenants/{tenantId}/projects/{projectId}/evaluations/datasets/{datasetId}
```

⚠️ **Warning**: Deleting a dataset will also delete all associated dataset items and may affect evaluation runs that reference this dataset.

## Managing Dataset Items

### List Items

```typescript
GET /tenants/{tenantId}/projects/{projectId}/evaluations/datasets/{datasetId}/items

// Optional query parameters
?limit=50&offset=0&metadata.category=returns
```

### Get Single Item

```typescript
GET /tenants/{tenantId}/projects/{projectId}/evaluations/datasets/{datasetId}/items/{itemId}
```

### Update Item

```typescript
PATCH /tenants/{tenantId}/projects/{projectId}/evaluations/datasets/{datasetId}/items/{itemId}

{
  "input": { /* updated input */ },
  "expectedOutput": { /* updated expected output */ }
}
```

### Delete Item

```typescript
DELETE /tenants/{tenantId}/projects/{projectId}/evaluations/datasets/{datasetId}/items/{itemId}
```

## Best Practices

### Dataset Organization

1. **Create focused datasets**: Group related test cases together (e.g., "Returns", "Technical Support", "Billing")
2. **Use descriptive names**: Make it clear what each dataset tests
3. **Add metadata**: Tag items for easy filtering and analysis
4. **Version control**: Keep track of dataset changes in your version control system

### Dataset Item Quality

1. **Diverse inputs**: Cover different phrasings, edge cases, and complexity levels
2. **Realistic scenarios**: Base test cases on actual user interactions
3. **Clear expected outputs**: Define specific, measurable success criteria
4. **Incremental difficulty**: Include easy, medium, and hard test cases

### Simulation Agents

1. **Specific personas**: Give simulation agents clear, specific personality traits
2. **Reasonable limits**: Set appropriate turn limits (5-10 turns typically)
3. **Test edge cases**: Use simulations to test difficult conversation paths
4. **Monitor costs**: Multi-turn simulations consume more LLM tokens

### Maintenance

1. **Regular updates**: Keep datasets current with product changes
2. **Remove obsolete tests**: Clean up outdated test cases
3. **Add failure cases**: When bugs are found, add them to your dataset
4. **Balance coverage**: Ensure datasets cover common and edge cases

## Next Steps

<Cards>
  <Card title="Evaluators" icon="LuScale" href="/typescript-sdk/evaluations/evaluators">
    Configure evaluators to judge agent responses
  </Card>
  <Card title="Running Evaluations" icon="LuPlay" href="/typescript-sdk/evaluations/running-evaluations">
    Execute evaluations on your datasets
  </Card>
  <Card title="Simulation Agents" icon="LuUsers" href="/typescript-sdk/evaluations/simulation-agents">
    Deep dive into multi-turn conversation testing
  </Card>
</Cards>

