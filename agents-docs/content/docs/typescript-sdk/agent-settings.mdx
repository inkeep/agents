---
title: Sub Agent Settings
description: Learn how to configure your Sub Agents
icon: "LuUser"
---

Sub Agents are the core building blocks of our framework, designed to be both powerful individual workers and collaborative team members in multi-agent systems. Through the framework's agent architecture, each Sub Agent can seamlessly delegate tasks, share context, and work together using structured data components.

## Creating a Sub Agent

Every Sub Agent needs an id, name, and clear prompt that define its behavior:

```typescript
import { subAgent } from "@inkeep/agents-sdk";

const supportAgent = subAgent({
  id: "customer-support",
  name: "Customer Support Agent",
  prompt: `You are a customer support specialist. Always be helpful, professional, and empathetic.`,
});
```

## Sub Agent Options

The framework supports rich Sub Agent configuration. Here are the options you can configure:

```typescript
const mySubAgent = subAgent({
  // Required
  id: "my-sub-agent-id", // Stable Sub Agent identifier
  name: "My Sub Agent", // Human-readable Sub Agent name provided to other Sub Agents which have a relationship with this Sub Agent
  prompt: "Detailed behavior guidelines", // System prompt. Not given to other Sub Agents which have a relationship with this Sub Agent

  // Optional - Sub Agent Description
  description: "Sub Agent description", // Brief description of the Sub Agent's purpose provided to other Sub Agents which have a relationship with this Sub Agent

  // Optional - Tools Integration
  canUse: () => [searchTool, mcpTool],

  // Optional - Sub Agent Relationships (for multi-agent systems)
  canTransferTo: () => [subAgent1], // Sub Agents this can hand off to
  canDelegateTo: () => [subAgent2], // Sub Agents this can delegate to

  // Optional - AI Model Settings (Vercel AI SDK v5)
  models: {
    base: {
      model: "anthropic/claude-sonnet-4-20250514",
      providerOptions: {
        temperature: 0.7,
        maxOutputTokens: 2048,   // Maximum tokens to generate
        maxDuration: 30,          // Timeout in seconds
        topP: 0.95,               // Nucleus sampling
        topK: 40,                 // Top-k sampling
        frequencyPenalty: 0.0,    // Reduce repetition
        presencePenalty: 0.0,     // Encourage new topics
        stopSequences: ["\n\n"], // Stop generation at sequences
        seed: 12345,              // For deterministic output
      },
    },
    structuredOutput: {
      model: "openai/gpt-4.1-mini-2025-04-14", // For structured JSON output
      providerOptions: {
        temperature: 0.1,
        maxOutputTokens: 1024,
        experimental_reasoning: true,  // Enable reasoning mode (if supported)
      },
    },
    summarizer: {
      model: "openai/gpt-4.1-nano-2025-04-14", // For summaries
      providerOptions: {
        temperature: 0.5,
        maxOutputTokens: 1000,
      },
    },
  },

  // Optional - Data Components (Structured Outputs)
  dataComponents: [
    {
      id: "customer-info",
      name: "CustomerInfo",
      description: "Customer information display component",
      props: {
        type: "object",
        properties: {
          name: { type: "string", description: "Customer name" },
          email: { type: "string", description: "Customer email" },
          issue: { type: "string", description: "Customer issue description" },
        },
        required: ["name", "email", "issue"],
      },
    },
  ],

  // Optional - Artifact Components (Structured Outputs from tools or Sub Agents)
  artifactComponents: [
    {
      id: "customer-info",
      name: "CustomerInfo",
      description: "Customer information display component",
      props: {
        type: "object",
        properties: {
          name: {
            type: "string",
            description: "Customer name",
            inPreview: true,
          },
          customer_info: {
            type: "string",
            description: "Customer information",
          },
        },
        required: ["name", "customer_info"],
      },
    },
  ],
});
```

| Parameter            | Type     | Required | Description                                                                                                                                                             |
| -------------------- | -------- | -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `id`                 | string   | Yes      | Stable Sub Agent identifier used for consistency and persistence                                                                                                            |
| `name`               | string   | Yes      | Human-readable name for the Sub Agent                                                                                                                                       |
| `prompt`             | string   | Yes      | Detailed behavior guidelines and system prompt for the Sub Agent                                                                                                            |
| `description`        | string   | No      | Brief description of the Sub Agent's purpose and capabilities                                                                                                               |
| `models`             | object   | No       | AI model settings with separate settings for base, structuredOutput, and summarizer models. <br /><br /> If no models settings are specified, the Sub Agent will inherit the models settings from its parent agent, which may inherit from the project settings.                                                                              |
| `models.base`        | object   | No       | Primary model for conversational text generation and reasoning                                                                                                          |
| `models.structuredOutput` | object | No       | Model used for structured JSON output only (falls back to base if not configured)                                                                                    |
| `models.summarizer`  | object   | No       | Model used for summaries and status updates (falls back to base if not configured)                                                                                     |
| `canUse`              | object   | No       | MCP tools that the Sub Agent can use. See [MCP Servers](/typescript-sdk/tools-and-mcp-servers) for details                                                                  |
| `dataComponents`     | array    | No       | Structured output components for rich, interactive responses. See [Data Components](/typescript-sdk/data-components) for details                                        |
| `artifactComponents` | array    | No       | Components for handling tool or Sub Agent outputs. See [Artifact Components](/typescript-sdk/artifact-components) for details                                               |
| `canTransferTo`      | function | No       | Function returning array of Sub Agents this Sub Agent can transfer to. See [Transfer Relationships](/typescript-sdk/agent-relationships#transfer-relationships) for details     |
| `canDelegateTo`      | function | No       | Function returning array of Sub Agents this Sub Agent can delegate to. See [Delegation Relationships](/typescript-sdk/agent-relationships#delegation-relationships) for details |

## Model Settings

The `models` object allows you to configure different models for different tasks, each with their own provider options:

```typescript
models: {
  base: {
    model: "anthropic/claude-sonnet-4-20250514", // Primary model for text generation
    providerOptions: {
      temperature: 0.7,
      maxOutputTokens: 2048  // AI SDK v5 uses maxOutputTokens
    }
  },
  structuredOutput: {
    model: "openai/gpt-4.1-mini-2025-04-14", // For structured JSON output only
    providerOptions: {
      temperature: 0.1,
      maxOutputTokens: 1024,
      experimental_reasoning: true  // Enable reasoning for better structured outputs
    }
  },
  summarizer: {
    model: "anthropic/claude-3-5-haiku-20241022", // For summaries and status updates
    providerOptions: {
      temperature: 0.5,
      maxOutputTokens: 1000
    }
  }
}
```

### Model Types

- **`base`**: Primary model used for conversational text generation and reasoning
- **`structuredOutput`**: Model used for structured JSON output only (falls back to base if not configured and nothing to inherit)
- **`summarizer`**: Model used for summaries and status updates (falls back to base if not configured and nothing to inherit)

### Supported Providers

The framework supports a wide range of models from major AI providers:

- **Anthropic**: For example `anthropic/claude-opus-4-1-20250805`, `anthropic/claude-sonnet-4.5-20250531`, `anthropic/claude-sonnet-4-20250514`, `anthropic/claude-3-5-haiku-20241022`, and more
- **OpenAI**: For example `openai/gpt-5-2025-08-07`, `openai/gpt-4.1-mini-2025-04-14`, `openai/gpt-4.1-nano-2025-04-14`, and more
- **Google**: For example `google/gemini-2.5-pro`, `google/gemini-2.5-flash`, `google/gemini-2.5-flash-lite`, and more
- **Additional providers** via OpenRouter and gateway routing

## Provider Options

All models support `providerOptions` to customize their behavior. These include both generic parameters that work across all providers and provider-specific features like reasoning.

### Generic Parameters

These parameters work with all supported providers and go directly in `providerOptions`:

```typescript
models: {
  base: {
    model: "anthropic/claude-sonnet-4-20250514",
    providerOptions: {
      maxOutputTokens: 4096,        // Maximum tokens to generate (AI SDK v5)
      temperature: 0.7,             // Controls randomness (0.0-1.0)
      topP: 0.95,                   // Nucleus sampling (0.0-1.0)
      topK: 40,                     // Top-k sampling (integer)
      frequencyPenalty: 0.0,        // Reduce repetition (-2.0 to 2.0)
      presencePenalty: 0.0,         // Encourage new topics (-2.0 to 2.0)
      stopSequences: ["\n\n"],     // Stop generation at sequences
      seed: 12345,                  // For deterministic output
      maxDuration: 30,              // Timeout in seconds (not milliseconds)
      maxRetries: 2,                // Maximum retry attempts
    }
  }
}
```

### Provider-Specific Features

Advanced features like reasoning require provider-specific configuration wrapped in the provider name:

#### OpenAI Reasoning
```typescript
models: {
  base: {
    model: "openai/o3-mini",
    providerOptions: {
      maxOutputTokens: 4096,
      temperature: 0.7,
      openai: {
        reasoningEffort: 'medium'  // 'low' | 'medium' | 'high'
      }
    }
  }
}
```

#### Anthropic Thinking
```typescript
models: {
  base: {
    model: "anthropic/claude-3-7-sonnet-20250219",
    providerOptions: {
      maxOutputTokens: 4096,
      temperature: 0.7,
      anthropic: {
        thinking: {
          type: 'enabled',
          budgetTokens: 8000  // Tokens allocated for reasoning
        }
      }
    }
  }
}
```

#### Google Gemini Thinking
```typescript
models: {
  base: {
    model: "google/gemini-2-5-flash",
    providerOptions: {
      maxOutputTokens: 4096,
      temperature: 0.7,
      google: {
        thinkingConfig: {
          thinkingBudget: 8192,     // 0 disables thinking
          includeThoughts: true     // Return thought summary
        }
      }
    }
  }
}
```

## Accessing Other Models

For models not directly supported, use these proxy providers:

- **OpenRouter**: Access any model via `openrouter/model-id` format (e.g., `openrouter/anthropic/claude-sonnet-4`, `openrouter/meta-llama/llama-3.1-405b`)
- **Vercel AI SDK Gateway**: Access models through your gateway via `gateway/model-id` format (e.g., `gateway/anthropic/claude-sonnet-4`)

```typescript
models: {
  base: {
    model: "openrouter/anthropic/claude-sonnet-4",
    providerOptions: {
      temperature: 0.7,
      maxOutputTokens: 2048
    }
  },
  structuredOutput: {
    model: "gateway/openai/gpt-4.1-mini",
    providerOptions: {
      maxOutputTokens: 1024
    }
  }
}
```

### Required API Keys

You need the appropriate API key for your chosen provider:

- `ANTHROPIC_API_KEY` for Anthropic models
- `OPENAI_API_KEY` for OpenAI models
- `GOOGLE_GENERATIVE_AI_API_KEY` for Google models
- `OPENROUTER_API_KEY` for OpenRouter models
- `AI_GATEWAY_API_KEY` for Vercel AI SDK Gateway models

### Default Models

When using the Inkeep CLI, the following defaults are applied based on your chosen provider:

**Anthropic:**
- `base`: `anthropic/claude-sonnet-4.5-20250531`
- `structuredOutput`: `anthropic/claude-sonnet-4.5-20250531`
- `summarizer`: `anthropic/claude-sonnet-4.5-20250531`

**OpenAI:**
- `base`: `openai/gpt-4.1-2025-04-14`
- `structuredOutput`: `openai/gpt-4.1-mini-2025-04-14`
- `summarizer`: `openai/gpt-4.1-nano-2025-04-14`

**Google:**
- `base`: `google/gemini-2.5-flash`
- `structuredOutput`: `google/gemini-2.5-flash-lite`
- `summarizer`: `google/gemini-2.5-flash-lite`

### Inheritance

If no `models` settings are specified, the Sub Agent will inherit the models settings from its parent agent, which may inherit from the project settings.

## Agent Prompt Integration

Sub Agents automatically receive any agent-level prompt configuration in addition to their individual prompt:

```typescript
// Agent-level prompt that gets added to all Sub Agents
const customerSupportAgent = agent({
  id: "support-agent",
  agentPrompt: `You work for Acme Corp. Always be professional and helpful.
Follow company policies and escalate complex issues appropriately.`,
  subAgents: () => [supportAgent, escalationAgent],
});
```

The `agentPrompt` is injected into each Sub Agent's system prompt, providing consistent context and behavior guidelines across all Sub Agents in the agent.

<Note>`openai/gpt-5-2025-08-07`, `openai/gpt-5-mini-2025-08-07`, and `openai/gpt-5-nano-2025-08-07` require a verified OpenAI organization. If your organization is not yet verified, these models will not be available.</Note>
