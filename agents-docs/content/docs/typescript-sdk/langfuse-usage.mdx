---
title: Using Langfuse for LLM Observability
sidebarTitle: Langfuse Usage
description: Complete guide to using Langfuse for LLM observability, tracing, and analytics in the Inkeep Agent Framework
keywords: Langfuse, LLM observability, tracing, OpenTelemetry, AI monitoring, token usage, model analytics
icon: "brand/Langfuse"
---

Langfuse is an open-source LLM engineering platform that provides specialized observability for AI applications, including token usage tracking, model performance analytics, and detailed LLM interaction tracing.

## Quick Start

### 1. Setup Langfuse Account

First, create a Langfuse account and get your API keys:

1. **Sign up** at [Langfuse Cloud](https://cloud.langfuse.com)
2. **Create a new project** in your Langfuse dashboard
3. **Get your API keys** from the project settings:
   - Public Key: `pk-lf-xxxxxxxxxx`
   - Secret Key: `sk-lf-xxxxxxxxxx`

### 2. Configure OpenTelemetry Collector

Add Langfuse as an exporter to your OTEL collector configuration:

```yaml
# otel-collector-config.yaml
exporters:
  # Export to Langfuse
  otlphttp/langfuse:
    endpoint: "https://us.cloud.langfuse.com/api/public/otel" # US region
    headers:
      Authorization: "Basic <BASE64_ENCODED_CREDENTIALS>"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlphttp/langfuse]
```

### 3. Generate Authentication Credentials

Langfuse requires Basic Authentication with base64-encoded credentials:

```bash
# Generate base64 encoded credentials
echo -n "pk-lf-YOUR_PUBLIC_KEY:sk-lf-YOUR_SECRET_KEY" | base64
```

### 4. Complete Configuration Example

Here's a complete OTEL collector configuration with Langfuse integration:

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 100ms
    send_batch_size: 1
    send_batch_max_size: 10

  attributes:
    actions:
      - key: http.request.header.authorization
        value: "[REDACTED]"
        action: update

exporters:
  # Export to Jaeger
  otlp/jaeger:
    endpoint: jaeger:4317
    tls:
      insecure: true

  # Export to SigNoz OTEL collector
  otlp/signoz:
    endpoint: signoz-otel-collector:4317
    tls:
      insecure: true

  # Export to Langfuse
  otlphttp/langfuse:
    endpoint: "https://us.cloud.langfuse.com/api/public/otel"
    headers:
      Authorization: "Basic XXX"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [attributes, batch]
      exporters: [otlp/jaeger, otlp/signoz, otlphttp/langfuse]
```

### 5. Start the Services

```bash
# From the root directory of the agent framework
cd deploy/docker
docker compose up -d

```

## Architecture

The Langfuse integration works alongside your existing observability stack:

```
Application → OTEL Collector → Jaeger → Jaeger UI (http://localhost:16686)
                             → SigNoz → SigNoz UI (http://localhost:3080)
                             → Langfuse → Langfuse Dashboard
```

## Running LLM Evaluations in Langfuse Dashboard

Langfuse provides a powerful web interface for running LLM evaluations without writing code. You can create datasets, set up evaluators, and run evaluations directly in the dashboard.

### Accessing the Evaluation Features

1. **Log into your Langfuse dashboard**: https://cloud.langfuse.com
2. **Navigate to your project** where your agent traces are being collected
3. **Click "Evaluations"** in the left sidebar
4. **Click "Set up evaluator"** to begin creating evaluations

### Setting Up LLM-as-a-Judge Evaluators

#### Set Up Default Evaluation Model

Before creating evaluators, you need to configure a default LLM connection for evaluations:

<img
  src="/images/langfuse-llm-connection.png"
  alt="Langfuse LLM Connection setup showing OpenAI provider configuration with API key field and advanced settings"
  width="100%"
  style={{ borderRadius: "10px", border: "1px solid #e1e5e9" }}
/>

**Setting up the LLM Connection:**

1. **Navigate to "Evaluator Library"** in your Langfuse dashboard
2. **Click "Set up"** next to "Default Evaluation Model"
3. **Configure the LLM connection**:
   - **LLM Adapter**: Select your preferred provider
   - **Provider Name**: Give it a descriptive name (e.g., "openai")
   - **API Key**: Enter your OpenAI API key (stored encrypted)
   - **Advanced Settings**: Configure base URL, model parameters if needed
4. **Click "Create connection"** to save

**Important Notes:**

- The default model is used by all managed evaluators
- You can change it anytime - existing evaluators will use the new model
- The model must support structured output

#### Navigate to Evaluator Setup

1. **Go to "Evaluations"** → **"Running Evaluators"**
2. **Click "Set up evaluator"** button
3. **You'll see two main steps**: "1. Select Evaluator" and "2. Run Evaluator"

#### Choose Your Evaluator Type

You have two main options:

##### Option A: Langfuse Managed Evaluators (Recommended)

Langfuse provides a comprehensive catalog of pre-built evaluators including **Quality & Accuracy Evaluators**, **RAG-Specific Evaluators**, and **Advanced Evaluators**

**To use a managed evaluator:**

1. **Browse the evaluator list** and find one that matches your needs
2. **Click on the evaluator** to see its description and criteria
3. **Click "Use Selected Evaluator"** button

##### Option B: Create Custom Evaluator

**Best for**: Specific evaluation needs not covered by managed evaluators

1. **Click "+ Create Custom Evaluator"** button

## Example Custom Evaluator with Weather Graph Example

### Step 1: Create a New Custom Evaluator

1. **Navigate to Evaluations** → **"Running Evaluators"**
2. **Click "Set up evaluator"** button
3. **Select "+ Create Custom Evaluator"**
4. **Configure the evaluator**:

<img
  src="/images/setup-evaluator.png"
  alt="Langfuse Create new evaluator dialog showing Prompt section with evaluation prompt, score reasoning prompt, and score range prompt fields"
  width="100%"
  style={{ borderRadius: "10px", border: "1px solid #e1e5e9" }}
/>


**Evaluation Prompt:**
```
You are an evaluator for a graph that utilizes a weather_info_tool to retrieve the weekly forecast for a specific location. Evaluate the {{output}} to determine if the agent is returning a 24 hour temperature forecast.
```

**Score Reasoning Prompt:**
```
Provide a detailed explanation of whether the {{output}} correctly contains a 24-hour temperature forecast. 
Mention key evidence from the text (e.g., presence of hourly temperatures for 24 hours, or lack thereof). 
Explain why this evidence supports a score of 1 (true/positive) or 0 (false/negative).
```

**Score Range Prompt:**
```
Score between 0 and 1. Score 0 if false or negative and 1 if true or positive.
```

### Step 2: Configure Evaluation Target and Scope

<img
  src="/images/setup-evaluator2.png"
  alt="Langfuse Set up evaluator screen showing target data selection, evaluator runs configuration, target filter options, sampling settings, delay configuration, and variable mapping section"
  width="100%"
  style={{ borderRadius: "10px", border: "1px solid #e1e5e9" }}
/>

**Target Data Selection:**
- Select **"Dataset runs"** for pre-defined test cases.

**Evaluator Runs On:**
- ✅ **New dataset run items**: Evaluate future weather graph executions
- ◻️ **Existing dataset run items**: Backfill historical evaluations (optional)

### Step 3: Variable Mapping

In the **Variable Mapping** section:

**Evaluation Prompt Variables:**
- **`{{output}}`** → Map to **"Trace"** → **"Output"**
  - This captures the final response from the weather graph

**Object Variable Mapping:**
- **Object**: `Trace`
- **Object Variable**: `Output`

### Step 4: Enable and Test the Evaluator

1. **Click "Save"** to create the evaluator
2. **Enable the evaluator** to start automatic evaluation
3. **Test with sample weather queries**:
   - "What's the weather like in San Francisco tomorrow?"
   - "Give me a 24-hour forecast for New York City"
   - "What will the temperature be in London over the next day?"



## Understanding Variable Mapping in Langfuse

Variable mapping is the process of connecting variables in your evaluation prompts (like `{{input}}`, `{{output}}`, `{{agentName}}`) to actual data from your traces. This allows your evaluators to access specific information from your agent executions.

### How to Discover Available Variables

Before creating variable mappings, you need to understand what data is available in your traces. Here's how to explore your trace data:

#### 1. Navigate to Your Traces

1. **Go to your Langfuse project dashboard**
2. **Click "Traces"** in the left sidebar
3. **Select a representative trace** from your agent graph execution
4. **Click on the trace** to view its detailed structure

#### 2. Explore Trace Structure

Your trace will contain several levels of data:

**Trace Level (Top-level execution):**
- `Input`: The original user query or request
- `Output`: The final response from your agent graph
- `Metadata`: Additional trace-level information
- `Tags`: Custom labels you've added

**Span Level (Individual agent/tool executions):**
- Each operation creates a span
- Spans contain their own input/output data
- Span metadata includes execution details
- Attributes contain structured data about the execution

#### 3. Inspect Span Attributes

Click on individual spans to see their attributes. Common attributes include:

```json
{
  "agent.name": "weather-assistant",
  "ai.toolCall.name": "weather_info_tool",
  "ai.toolType": "delegation",
  "service.name": "inkeep-agents"
}
```

### Variable Mapping Syntax

When mapping variables, you use different object types and paths:

#### Object Types

- **Trace**: Top-level execution data
- **Span**: Individual agent/tool execution data

#### Variable Paths

- **Direct properties**: `Input`, `Output`, `Metadata`
- **JSON paths**: `$.attributes["agent.name"]` for nested data
- **Array access**: `$.toolCalls[0].parameters` for array elements

### Common Variable Mapping Patterns

#### Basic Trace Variables
```
{{input}} → Trace → Input
{{output}} → Trace → Output  
```

#### Agent Information from Spans
```
{{agentName}} → Span → Metadata → $.attributes["agent.name"]
{{toolUsed}} → Span → Metadata → $.attributes["ai.toolCall.name"]
```

This comprehensive variable mapping system allows you to create sophisticated evaluators that can assess any aspect of your agent's performance by accessing the rich trace data collected by Langfuse.
