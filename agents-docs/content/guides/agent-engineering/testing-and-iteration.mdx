---
title: Testing, Debugging, and Updating Agents
sidebarTitle: Testing & Iteration
description: Debug agent underperformance, distinguish designer failures from runtime failures, tune agent behavior, and update existing agents without drift.
---

Agent engineering doesn't stop at writing the first prompt. This page covers how to debug underperformance, tune behavior, and update agents while preserving their original intent.

## Diagnosing underperformance

When an agent fails, the first step is identifying whether the problem is in **your prompt** or in **the model's execution**:

| Category | What it means | Examples | Fix |
|---|---|---|---|
| **Designer failure** | The prompt has issues | Ambiguous instructions, missing escalation rules, no output format | Edit the prompt |
| **Runtime failure** | The prompt is sound but the model misapplies it | Model ignores clear instruction, hallucinates despite guardrails | Strengthen emphasis, add examples, or accept limitation |

**Rule of thumb:** If you can imagine a careful human reader misinterpreting the prompt the same way the model did, it's a designer failure.

### Diagnostic questions

1. If you gave this prompt to a different model, would it interpret it the same way?
2. Are there instructions that could be read two ways?
3. Does the agent have enough context to judge edge cases?
4. Is the output format specific enough to evaluate whether output is "good"?

If any answer is "no," it's likely a designer failure. Fix the prompt before blaming the model.

## Common prompt design issues

### 1. Ambiguous instructions

Instructions that could be read two ways — the model picks one interpretation; you expected the other.

| Ambiguous | Clear |
|---|---|
| "Be thorough" | "Check for security issues, missing error handling, and breaking API changes" |
| "Clean up the code" | "Remove unused imports and dead code paths" |
| "Review for issues" | "Review for security vulnerabilities and correctness bugs" |

**Fix:** Add clarifying examples, "do X, not Y" constraints, or explicit scope.

### 2. Missing context

The prompt assumes context that won't be available at runtime.

| Problem | Fix |
|---|---|
| "Continue from where we left off" | Include all needed context in the prompt or handoff |
| "Apply the pattern we discussed" | Specify the pattern explicitly |
| "Fix the issue" | Describe the issue in detail |

This is especially important for **delegate Sub Agents**, which receive only what the coordinator passes them — they don't share conversation history.

### 3. Vague directive strength

Using weak language for requirements or strong language for preferences.

| Problem | Better |
|---|---|
| "Try to run tests" (when tests are required) | "You must run tests before returning" |
| "Consider security" (when security is non-negotiable) | "You must check for security vulnerabilities" |
| "Must follow style guide" (for cosmetic preferences) | "You should follow the style guide when possible" |

### 4. Missing troubleshooting guardrails

The prompt tells the agent what to do, but not what to check for when behavior drifts.

**Fix:** Include 3-5 contextually relevant checks from [Prompt Troubleshooting](/guides/agent-engineering/prompt-troubleshooting).

### 5. Underspecified output format

The prompt doesn't define what "good output" looks like.

| Vague | Specific |
|---|---|
| "Return your findings" | "Return findings as a prioritized list with severity, file path, description, and recommendation" |
| "Summarize the issues" | "Return a TL;DR (2-5 bullets) followed by findings sorted by severity" |

### 6. Missing escalation rules

No guidance on when to ask for help vs proceed with assumptions.

**Fix:** Add explicit rules: when to ask, when to proceed with labeled assumptions, when to return partial results.

### 7. Overloaded scope

Too many instructions competing for attention. The model's attention degrades with context length, so a prompt stuffed with low-priority instructions can cause it to miss the important ones.

**Fix:** Prioritize ruthlessly. Use "must" for critical items and "should" for nice-to-haves. Move reference material to tools or data sources the agent can query on demand.

### 8. False dichotomies

Presenting two options when more exist.

| False dichotomy | Better |
|---|---|
| "Either fix the bug or document it" | "Fix, document, or escalate based on severity and confidence" |
| "Ask or assume" | "Ask for high-stakes decisions; proceed with labeled assumptions for low-stakes ones" |

## The designer self-check

Before deploying a prompt, verify:

- [ ] No instruction could be read two ways in a different context
- [ ] No assumed context the agent won't have
- [ ] Directive strength is clear throughout ("must" / "should" / "consider")
- [ ] 3-5 relevant troubleshooting checks are addressed
- [ ] Output format is specific enough to validate against
- [ ] Escalation rules are clear
- [ ] Scope is manageable (the agent can hold all critical instructions in effective attention)

## Tuning agent behavior

### If the agent's output is too verbose

- Strengthen the output format with explicit verbosity limits
- Add a conciseness rule (e.g., "max 1-2 screens unless asked for more")
- Tell the agent to lead with findings, not preamble

### If it misses key checks

- Add a required checklist item ("Always run X", "Always verify Y")
- Make the check explicit in the workflow section of the prompt

### If it overreaches or changes too much

- Tighten scope and non-goals in the prompt
- Reduce tool access (don't give write tools to a reviewer)
- Make the boundary explicit: "Do not modify files outside the target scope"

### If transfers or delegation aren't happening correctly

- Verify the Sub Agent descriptions are clear about what each specialist handles
- Add concrete examples of when to transfer vs delegate in the coordinator's prompt
- Check that the coordinator prompt explicitly names the routing logic

### If delegate Sub Agents return unhelpful results

- Ensure the coordinator passes enough context when delegating
- Add an explicit output format to the specialist's prompt
- Check that the specialist's prompt is self-contained (doesn't assume prior conversation)

## Updating existing agents

When updating an existing agent, the goal is to improve clarity and structure **without changing the agent's meaning** unless explicitly intended.

### Safe changes

These don't affect meaning or capability:

- Reformatting (headings, lists, checklists) without changing requirements
- Reducing redundancy and merging duplicated text
- Reordering sections for scannability
- Tightening language where intent is already clear
- Fixing typos, grammar, or formatting inconsistencies

### Changes that need careful review

These can affect when the agent acts or how it judges:

- Expanding or narrowing Sub Agent descriptions
- Adding or removing examples from the prompt
- Changing MUST/SHOULD wording or severity levels
- Modifying personality statements or escalation thresholds
- Adding or removing troubleshooting guardrails

### Changes that can break downstream consumers

These can break coordinators, integrations, or callers that depend on the agent's behavior:

- Renaming a Sub Agent (breaks transfer/delegate references)
- Changing output format (breaks coordinator aggregation)
- Changing severity levels or dedup keys
- Switching a Sub Agent's role from specialist to coordinator (or vice versa)

### Common anti-patterns when updating

| Anti-pattern | Description |
|---|---|
| **Oversampling** | Adding rules or prompt checks "because it seems good" — only add what's actually needed |
| **Tone normalization** | Rewriting personality into your own style, shifting strictness thresholds |
| **Calibration creep** | Subtly shifting MUST to SHOULD without realizing the behavioral impact |
| **Tool accumulation** | Adding tools "for convenience" without considering the expanded risk surface |
| **Format drift** | Changing output format when a coordinator depends on it |

## Maintenance best practices

- Prefer small, targeted edits based on observed failures
- Test agent behavior after changes to descriptions or prompt examples
- When multiple failures occur, make targeted fixes rather than shotgun edits across the prompt
- Keep a changelog in version control rather than inside the prompt itself
